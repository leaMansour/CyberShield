{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay  \n"
      ],
      "metadata": {
        "id": "xCGtSqCPKQNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "tweet_df = pd.read_csv(\"labeled_data.csv\")\n",
        "\n",
        "# data processing\n",
        "def data_processing(tweet):\n",
        "    tweet = tweet.lower()\n",
        "    tweet = re.sub(r\"https\\S+|www\\S+http\\S+\", '', tweet, flags = re.MULTILINE)\n",
        "    tweet = re.sub(r'\\@w+|\\#','', tweet)\n",
        "    tweet = re.sub(r'[^\\w\\s]','',tweet)\n",
        "    tweet = re.sub(r'รฐ','',tweet)\n",
        "    tweet_tokens = word_tokenize(tweet)\n",
        "    filtered_tweets = [w for w in tweet_tokens if not w in stop_words]\n",
        "    return \" \".join(filtered_tweets)\n",
        "tweet_df.tweet = tweet_df['tweet'].apply(data_processing)\n",
        "tweet_df = tweet_df.drop_duplicates('tweet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizing(data):\n",
        "    tweet = [lemmatizer.lemmatize(word) for word in data]\n",
        "    return data\n",
        "tweet_df['tweet'] = tweet_df['tweet'].apply(lambda x: lemmatizing(x))"
      ],
      "metadata": {
        "id": "m6YhIxNqixtQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvgGxL_zhwn8"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(ngram_range=(1,3)).fit(tweet_df['tweet'])\n",
        "\n",
        "# retrieving the features and target \n",
        "X= tweet_df['tweet']\n",
        "Y= tweet_df['hate_speech']\n",
        "X= vect.transform(X)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "logreg = LogisticRegression()\n",
        "param_grid = {'C':[100, 10, 1.0, 0.1, 0.01], 'solver' :['newton-cg', 'lbfgs','liblinear']}\n",
        "grid = GridSearchCV(LogisticRegression(), param_grid, cv = 5)\n",
        "grid.fit(x_train, y_train)\n",
        "print(\"Best Cross validation score: {:.2f}\".format(grid.best_score_))\n",
        "print(\"Best parameters: \", grid.best_params_)\n",
        "y_pred = grid.predict(x_test)\n",
        "logreg_acc = accuracy_score(y_pred, y_test)\n",
        "print(\"Test accuracy: {:.2f}%\".format(logreg_acc*100))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "test = \"fuck you\"\n",
        "test = vect.transform([test]).toarray()\n",
        "if grid.predict(test) != 0: \n",
        "  print('Cyberbullying is detected.')\n",
        "else: \n",
        "  print('No cyberbullying is detected.')\n",
        "\n",
        "test = \"you are cute\"\n",
        "test = vect.transform([test]).toarray()\n",
        "if grid.predict(test) != 0: \n",
        "  print('Cyberbullying is detected.')\n",
        "else: \n",
        "  print('No cyberbullying is detected.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k93_jvmoJ-5N",
        "outputId": "79280245-30a3-472d-81ec-72d400df205d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cyberbullying is detected.\n",
            "No cyberbullying is detected.\n"
          ]
        }
      ]
    }
  ]
}